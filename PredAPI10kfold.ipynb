{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predicting product ratings from Amazon reviews using Google Prediction API#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using 10 fold cross validation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the various libraries to be used ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to read a json file as a list of json objects###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the text submitted to the Google prediction API has to be dense, we convert the raw product reviews into a bag of words by removing the punctuations and reducing each word to its root form (stemming)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    m = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. add stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    meaningful_words = []\n",
    "    for w in m:\n",
    "      w = stemmer.stem(w)\n",
    "      meaningful_words.append(w)\n",
    "      \n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def catArray_to_text(category):\n",
    "    #Function to convert the category from a 2d array to a string with formatting\n",
    "    cat = np.asarray(category)\n",
    "    newcat = []\n",
    "    for arr in cat:\n",
    "        newcat.append(\"-\".join(arr))\n",
    "        newcats = [categori.replace(' ', '_') for categori in newcat]\n",
    "    return (\" \".join(newcats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list(parseData(\"data.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Split the data into different ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate1 = []\n",
    "rate2 = []\n",
    "rate3 = []\n",
    "rate4 = []\n",
    "rate5 = []\n",
    "for d in data:\n",
    "    if float(d['rating']) == 1.0:\n",
    "        rate1.append(d)\n",
    "    elif float(d['rating']) == 2.0:\n",
    "        rate2.append(d)\n",
    "    elif float(d['rating']) == 3.0:\n",
    "        rate3.append(d)\n",
    "    elif float(d['rating']) == 4.0:\n",
    "        rate4.append(d)\n",
    "    elif float(d['rating']) == 5.0:\n",
    "        rate5.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create a new dataset with equal distribution of all ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newd = []\n",
    "newd.extend(rate1[:20000])\n",
    "newd.extend(rate2[:20000])\n",
    "newd.extend(rate3[:20000])\n",
    "newd.extend(rate4[:20000])\n",
    "newd.extend(rate5[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "newd = shuffle(newd, random_state=2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open the training/testing file in which the modified data has to be written\n",
    "\n",
    "def formatFile(fileArray):\n",
    "    #chane the name of the file you want to write to\n",
    "    train_file = open(\"put_appropriate_name_here.json\", 'w')\n",
    "    for l in fileArray:\n",
    "      itemID,rating,helpful,reviewText,reviewerID,summary,unixReviewTime,category = \\\n",
    "      l['itemID'],l['rating'],l['helpful'],l['reviewText'],l['reviewerID'],l['summary'],l['unixReviewTime'],l['category']\n",
    "      #count the helpfulness of the review from the given attributes 'nHelpful' and 'outOf'\n",
    "      helpfulness = int(helpful['nHelpful'])*1.0/int(helpful['outOf'])\n",
    "      #pre-process the review text and the categories\n",
    "      clean_review = review_to_words( reviewText )\n",
    "      clean_cat = catArray_to_text(category)\n",
    "      #write the clean json object to file\n",
    "      train_file.write('{' + \"'rating': \"          + str(rating)                  + ', ' \n",
    "                            + \"'itemID': \"          + '\"' + itemID + '\"'           + ', '\n",
    "                            + \"'helpful': \"         + str(helpfulness)             + ', ' \n",
    "                            + \"'text': \"            + '\"' + clean_review + '\"'     + ', '\n",
    "                            + \"'reviewerID': \"      + '\"' + reviewerID + '\"'       + ', '\n",
    "                            + \"'categories': \"      + '\"' + clean_cat + '\"'        + ', '\n",
    "                            + \"'unixReviewTime': \"  + str(unixReviewTime)\n",
    "                            + '}' + '\\n')\n",
    "\n",
    "    train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "kf = cross_validation.KFold(len(newd), n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the train/test split for each iteration of the crossvalidation\n",
    "i = 0\n",
    "train = []\n",
    "test = []    \n",
    "for train_ind, test_ind in kf:\n",
    "    training = []\n",
    "    testing = []\n",
    "    for index in train_ind:\n",
    "        training.append(newd[index])\n",
    "    for index in test_ind:\n",
    "        testing.append(newd[index])\n",
    "    train.append(training)\n",
    "    test.append(testing)\n",
    "    i+=1   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formatFile(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(train[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatFile(test[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the training and the test file according to the required format of the API. The first column is the product ratings, i.e., the labels and the remaining columns are the different features###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, once the data is prepared, upload the files to google prediction API and run the command line script to create a prediction file where the first row is the actual rating and the second row is the predicted value.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running the script and preparing the prediction file, calculate the accuracy of your predictions.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Print out the results: Accuracy, Mean Squared Error, Confusion Matrix###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_0.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 5474\n",
      "accuracy is  45.26\n",
      "Below is the confusion matrix\n",
      "[0, 365, 167, 128, 137]\n",
      "[635, 0, 394, 227, 186]\n",
      "[273, 388, 0, 526, 306]\n",
      "[91, 164, 266, 0, 681]\n",
      "[75, 71, 80, 314, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_1.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 5630\n",
      "accuracy is  43.7\n",
      "Below is the confusion matrix\n",
      "[0, 353, 249, 127, 154]\n",
      "[628, 0, 503, 277, 215]\n",
      "[269, 278, 0, 567, 287]\n",
      "[68, 120, 323, 0, 595]\n",
      "[61, 48, 115, 393, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_2.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  9054\n",
      "Total number of incorrect predictions 5219\n",
      "accuracy is  42.36\n",
      "Below is the confusion matrix\n",
      "[0, 374, 192, 139, 144]\n",
      "[423, 0, 436, 261, 184]\n",
      "[218, 314, 0, 487, 236]\n",
      "[79, 140, 352, 0, 556]\n",
      "[85, 72, 147, 380, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_3.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  8340\n",
      "Total number of incorrect predictions 4671\n",
      "accuracy is  43.99\n",
      "Below is the confusion matrix\n",
      "[0, 300, 208, 95, 114]\n",
      "[447, 0, 408, 235, 155]\n",
      "[219, 252, 0, 516, 232]\n",
      "[67, 79, 269, 0, 509]\n",
      "[66, 45, 115, 340, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_4.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 5577\n",
      "accuracy is  44.23\n",
      "Below is the confusion matrix\n",
      "[0, 424, 245, 131, 164]\n",
      "[538, 0, 461, 223, 184]\n",
      "[270, 350, 0, 478, 271]\n",
      "[118, 138, 337, 0, 541]\n",
      "[93, 93, 145, 373, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_5.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  8704\n",
      "Total number of incorrect predictions 5042\n",
      "accuracy is  42.07\n",
      "Below is the confusion matrix\n",
      "[0, 386, 207, 132, 104]\n",
      "[436, 0, 391, 218, 167]\n",
      "[215, 323, 0, 457, 224]\n",
      "[102, 137, 340, 0, 531]\n",
      "[91, 83, 156, 342, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_6.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  3610\n",
      "Total number of incorrect predictions 2005\n",
      "accuracy is  44.46\n",
      "Below is the confusion matrix\n",
      "[0, 151, 80, 50, 45]\n",
      "[175, 0, 177, 90, 64]\n",
      "[95, 133, 0, 191, 73]\n",
      "[36, 56, 133, 0, 197]\n",
      "[35, 24, 60, 140, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_7.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 5659\n",
      "accuracy is  43.41\n",
      "Below is the confusion matrix\n",
      "[0, 353, 231, 123, 160]\n",
      "[571, 0, 455, 303, 215]\n",
      "[252, 306, 0, 615, 233]\n",
      "[91, 116, 284, 0, 671]\n",
      "[68, 42, 104, 466, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_8.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 5656\n",
      "accuracy is  43.44\n",
      "Below is the confusion matrix\n",
      "[0, 314, 250, 153, 154]\n",
      "[565, 0, 555, 270, 214]\n",
      "[229, 252, 0, 601, 272]\n",
      "[87, 96, 338, 0, 678]\n",
      "[73, 42, 124, 389, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_9.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    #print(l.split())\n",
    "    num = num+1\n",
    "    actual,pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 5530\n",
      "accuracy is  44.7\n",
      "Below is the confusion matrix\n",
      "[0, 366, 243, 131, 117]\n",
      "[574, 0, 466, 281, 168]\n",
      "[254, 323, 0, 536, 233]\n",
      "[92, 108, 343, 0, 600]\n",
      "[75, 48, 167, 405, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((1 - err*1.0/num)*100,2)\n",
    "acc.append(accuracy)\n",
    "print ('Total number of predictions made: ' , num) \n",
    "print ('Total number of incorrect predictions' , err)\n",
    "print ('accuracy is ' , accuracy)\n",
    "print ('Below is the confusion matrix')\n",
    "print (oneErr)\n",
    "print (twoErr)\n",
    "print (threeErr)\n",
    "print (fourErr)\n",
    "print (fiveErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of the model is 43.762\n"
     ]
    }
   ],
   "source": [
    "print ('The average accuracy of the model is' ,np.mean(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
