{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predicting product ratings from Amazon reviews using Google Prediction API#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the various libraries to be used ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to read a json file as a list of json objects###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the text submitted to the Google prediction API has to be dense, we convert the raw product reviews into a bag of words by removing the punctuations and reducing each word to its root form (stemming)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    m = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. add stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    meaningful_words = []\n",
    "    for w in m:\n",
    "      w = stemmer.stem(w)\n",
    "      meaningful_words.append(w)\n",
    "      \n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def catArray_to_text(category):\n",
    "    #Function to convert the category from a 2d array to a string with formatting\n",
    "    cat = np.asarray(category)\n",
    "    newcat = []\n",
    "    for arr in cat:\n",
    "        newcat.append(\"-\".join(arr))\n",
    "        newcats = [categori.replace(' ', '_') for categori in newcat]\n",
    "    return (\" \".join(newcats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list(parseData(\"data.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test set. Since the prediction API recommends to use as big a training set possible, out of the 1 million data points, I use 990,000 data points for training the model and 10,000 data points for testing the model. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data[:990000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = data[990000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the training and the test file according to the required format of the API. The first column is the product ratings, i.e., the labels and the remaining columns are the different features###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open the training file in which the modified data has to be written\n",
    "train_file = open(\"train.json\", 'w')\n",
    "\n",
    "for l in train:\n",
    "  itemID,rating,helpful,reviewText,reviewerID,summary,unixReviewTime,category = \\\n",
    "  l['itemID'],l['rating'],l['helpful'],l['reviewText'],l['reviewerID'],l['summary'],l['unixReviewTime'],l['category']\n",
    "  \n",
    "  #count the helpfulness of the review from the given attributes 'nHelpful' and 'outOf'\n",
    "  helpfulness = int(helpful['nHelpful'])*1.0/int(helpful['outOf'])\n",
    "  #pre-process the review text\n",
    "  clean_review = review_to_words( reviewText )\n",
    "  #for Approach 1, categories is not used\n",
    "  clean_cat = catArray_to_text(category)\n",
    "  #write the clean json object to file\n",
    "  train_file.write('{' + \"'rating': \"          + str(rating)                  + ', ' \n",
    "                        + \"'itemID': \"          + '\"' + itemID + '\"'           + ', '\n",
    "                        + \"'helpful': \"         + str(helpfulness)             + ', ' \n",
    "                        + \"'text': \"            + '\"' + clean_review + '\"'     + ', '\n",
    "                        + \"'reviewerID': \"      + '\"' + reviewerID + '\"'       + ', '\n",
    "                        # for Approach 1, remove categories\n",
    "                        + \"'categories': \"      + '\"' + clean_cat + '\"'        + ', '\n",
    "                        + \"'unixReviewTime': \"  + str(unixReviewTime)\n",
    "                        + '}' + '\\n')\n",
    "\n",
    "train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open the test file in which the modified data has to be written\n",
    "test_file = open(\"test.json\", 'w')\n",
    "\n",
    "for l in test:\n",
    "  itemID,rating,helpful,reviewText,reviewerID,summary,unixReviewTime,category = l['itemID'],l['rating'],l['helpful'],l['reviewText'],l['reviewerID'],l['summary'],l['unixReviewTime'],l['category']\n",
    "    \n",
    "  #count the helpfulness of the review from the given attributes 'nHelpful' and 'outOf'\n",
    "  helpfulness = int(helpful['nHelpful'])*1.0/int(helpful['outOf'])\n",
    "    \n",
    "  #pre-process the review text\n",
    "  clean_review = review_to_words( reviewText ) \n",
    "  clean_cat = catArray_to_text(category)  \n",
    "  #write the clean json object to file\n",
    "  test_file.write('{' + \"'rating': \"          + str(rating)                  + ', ' \n",
    "                        + \"'itemID': \"          + '\"' + itemID + '\"'           + ', '\n",
    "                        + \"'helpful': \"         + str(helpfulness)             + ', ' \n",
    "                        + \"'text': \"            + '\"' + clean_review + '\"'     + ', '\n",
    "                        + \"'reviewerID': \"      + '\"' + reviewerID + '\"'       + ', '\n",
    "                        + \"'categories': \"      + '\"' + clean_cat + '\"'        + ', '\n",
    "                        + \"'unixReviewTime': \"  + str(unixReviewTime)\n",
    "                        + '}' + '\\n')\n",
    "\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, once the data is prepared, upload the files to google prediction API and run the command line script to create a prediction file where the first row is the actual rating and the second row is the predicted value.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running the script and preparing the prediction file, calculate the accuracy of your predictions.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "err = 0\n",
    "sqErr = 0\n",
    "oneErr =   [0,0,0,0,0]\n",
    "twoErr =   [0,0,0,0,0]\n",
    "threeErr = [0,0,0,0,0]\n",
    "fourErr =  [0,0,0,0,0]\n",
    "fiveErr =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    num = num+1\n",
    "    actual, pred = l.split()\n",
    "    actual = float(actual)\n",
    "    pred = float(pred)\n",
    "    if actual != pred:\n",
    "        err = err+1\n",
    "        if actual == 1.0:\n",
    "            oneErr[int(pred) - 1] += 1\n",
    "        elif actual == 2.0:\n",
    "            twoErr[int(pred) - 1] += 1\n",
    "        elif actual == 3.0:\n",
    "            threeErr[int(pred) - 1] += 1\n",
    "        elif actual == 4.0:\n",
    "            fourErr[int(pred) - 1] += 1\n",
    "        else:\n",
    "            fiveErr[int(pred) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Print out the results: Accuracy, Mean Squared Error, Confusion Matrix###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  9999\n",
      "Total number of incorrect predictions 3548\n",
      "accuracy is  64.52\n",
      "Below is the confusion matrix\n",
      "[0, 41, 67, 41, 133]\n",
      "[334, 0, 129, 98, 139]\n",
      "[190, 57, 0, 262, 307]\n",
      "[72, 22, 145, 0, 1082]\n",
      "[83, 20, 48, 278, 0]\n"
     ]
    }
   ],
   "source": [
    "print 'Total number of predictions made: ' , num\n",
    "print 'Total number of incorrect predictions' , err\n",
    "print 'accuracy is ' , round((1 - err*1.0/num)*100,2)\n",
    "print 'Below is the confusion matrix'\n",
    "print oneErr\n",
    "print twoErr\n",
    "print threeErr\n",
    "print fourErr\n",
    "print fiveErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num1 = 0\n",
    "err1 = 0\n",
    "sqErr1 = 0\n",
    "oneErr1 =   [0,0,0,0,0]\n",
    "twoErr1 =   [0,0,0,0,0]\n",
    "threeErr1 = [0,0,0,0,0]\n",
    "fourErr1 =  [0,0,0,0,0]\n",
    "fiveErr1 =  [0,0,0,0,0]\n",
    "predictionsFile = open(\"predictions_withCat.txt\", 'r')\n",
    "for l in predictionsFile:\n",
    "    num1 = num1+1\n",
    "    actual1, pred1 = l.split()\n",
    "    actual1 = float(actual1)\n",
    "    pred1 = float(pred1)\n",
    "    if actual1 != pred1:\n",
    "        err1 = err1+1\n",
    "        if actual1 == 1.0:\n",
    "            oneErr1[int(pred1) - 1] += 1\n",
    "        elif actual1 == 2.0:\n",
    "            twoErr1[int(pred1) - 1] += 1\n",
    "        elif actual1 == 3.0:\n",
    "            threeErr1[int(pred1) - 1] += 1\n",
    "        elif actual1 == 4.0:\n",
    "            fourErr1[int(pred1) - 1] += 1\n",
    "        else:\n",
    "            fiveErr1[int(pred1) - 1] += 1\n",
    "predictionsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions made:  10000\n",
      "Total number of incorrect predictions 3637\n",
      "accuracy is  63.63\n",
      "Below is the confusion matrix\n",
      "[0, 60, 69, 45, 165]\n",
      "[276, 0, 157, 83, 161]\n",
      "[162, 79, 0, 214, 352]\n",
      "[59, 43, 162, 0, 1133]\n",
      "[66, 34, 76, 241, 0]\n"
     ]
    }
   ],
   "source": [
    "print 'Total number of predictions made: ' , num1\n",
    "print 'Total number of incorrect predictions' , err1\n",
    "print 'accuracy is ' , round((1 - err1*1.0/num1)*100,2)\n",
    "print 'Below is the confusion matrix'\n",
    "print oneErr1\n",
    "print twoErr1\n",
    "print threeErr1\n",
    "print fourErr1\n",
    "print fiveErr1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
